{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tables\n",
      "  Downloading tables-3.6.1-cp36-cp36m-manylinux1_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numexpr>=2.6.2\n",
      "  Downloading numexpr-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (162 kB)\n",
      "\u001b[K     |████████████████████████████████| 162 kB 135.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.3 in /home/hq16960/anaconda3/envs/py36/lib/python3.6/site-packages (from tables) (1.18.1)\n",
      "Installing collected packages: numexpr, tables\n",
      "Successfully installed numexpr-2.7.1 tables-3.6.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting uni domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unis = pd.read_csv('unis_v2.csv')\n",
    "unique_domains = df_unis['domain'].unique()\n",
    "domain_size = len(unique_domains)\n",
    "\n",
    "d_time = datetime.datetime(2019, 12, 30, 12, 4, 5)\n",
    "end_date = datetime.datetime(2020, 4, 1, 12, 4, 5).date()\n",
    "\n",
    "search_domains = []\n",
    "search_d = ''\n",
    "for index in range(domain_size):\n",
    "    if index != 0 and (index % 30 == 0 or index == domain_size - 1):\n",
    "        search_domains.append(search_d[:-4])\n",
    "        search_d = ''\n",
    "    else:\n",
    "        dom = unique_domains[index]\n",
    "        if type(dom) == str:\n",
    "            search_d += 'site:{} OR '.format(dom)\n",
    "\n",
    "# data = {}\n",
    "# date = d_time.date()\n",
    "\n",
    "# while date < end_date:\n",
    "# #     date1 = d_time.date()\n",
    "#     d_time += datetime.timedelta(days=1)\n",
    "#     date = d_time.date()\n",
    "#     urls = []\n",
    "#     print(date1, date)\n",
    "\n",
    "#     for domains in search_domains:\n",
    "#         query = 'Covid-19 ' + domains\n",
    "#         results = search(query,None,  num=50, start=0, stop=100, pause=2, tbs=get_tbs(date, date))\n",
    "#         for url in results:\n",
    "#             urls.append(url)\n",
    "#     data[date] = urls\n",
    "#     with open('output_{}.json'.format(date), 'w') as f:\n",
    "#     # this would place the entire output on one line\n",
    "#     # use json.dump(lista_items, f, indent=4) to \"pretty-print\" with four spaces per indent\n",
    "#         json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['site:abdn.ac.uk OR site:aber.ac.uk OR site:anglia.ac.uk OR site:aston.ac.uk OR site:bangor.ac.uk OR site:bathspa.ac.uk OR site:beds.ac.uk OR site:bbk.ac.uk OR site:bcu.ac.uk OR site:bolton.ac.uk OR site:bournemouth.ac.uk OR site:brad.ac.uk OR site:brighton.ac.uk OR site:bris.ac.uk OR site:brunel.ac.uk OR site:buckingham.ac.uk OR site:cam.ac.uk OR site:canterbury.ac.uk OR site:cardiff.ac.uk OR site:chester.ac.uk OR site:chi.ac.uk OR site:city.ac.uk OR site:coventry.ac.uk OR site:cranfield.ac.uk OR site:ucreative.ac.uk OR site:cumbria.ac.uk OR site:dmu.ac.uk OR site:derby.ac.uk OR site:dundee.ac.uk',\n",
       " 'site:uea.ac.uk OR site:uwl.ac.uk OR site:edgehill.ac.uk OR site:ed.ac.uk OR site:essex.ac.uk OR site:exeter.ac.uk OR site:falmouth.ac.uk OR site:gla.ac.uk OR site:gcal.ac.uk OR site:glos.ac.uk OR site:glyndwr.ac.uk OR site:gold.ac.uk OR site:gre.ac.uk OR site:harper-adams.ac.uk OR site:hw.ac.uk OR site:herts.ac.uk OR site:hud.ac.uk OR site:hull.ac.uk OR site:ucl.ac.uk OR site:keele.ac.uk OR site:ukc.ac.uk OR site:kcl.ac.uk OR site:kingston.ac.uk OR site:uclan.ac.uk OR site:lancs.ac.uk OR site:leeds.ac.uk OR site:leedsbeckett.ac.uk OR site:leedstrinity.ac.uk OR site:lincoln.ac.uk',\n",
       " 'site:hope.ac.uk OR site:ljmu.ac.uk OR site:lon.ac.uk OR site:londonmet.ac.uk OR site:lshtm.ac.uk OR site:lse.ac.uk OR site:lsbu.ac.uk OR site:lut.ac.uk OR site:man.ac.uk OR site:mmu.ac.uk OR site:mdx.ac.uk OR site:ncl.ac.uk OR site:northampton.ac.uk OR site:nua.ac.uk OR site:nottingham.ac.uk OR site:ntu.ac.uk OR site:ox.ac.uk OR site:brookes.ac.uk OR site:yorksj.ac.uk OR site:york.ac.uk OR site:writtle.ac.uk OR site:worc.ac.uk OR site:wlv.ac.uk OR site:winchester.ac.uk OR site:westminster.ac.uk OR site:uws.ac.uk OR site:uwe.ac.uk OR site:.warwick.ac.uk OR site:ulst.ac.uk',\n",
       " 'site:sussex.ac.uk OR site:surrey.ac.uk OR site:sunderland.ac.uk OR site:strath.ac.uk OR site:stir.ac.uk OR site:staffs.ac.uk OR site:stmarys.ac.uk OR site:sgul.ac.uk OR site:st-and.ac.uk OR site:southampton.ac.uk OR site:shu.ac.uk OR site:shef.ac.uk OR site:salford.ac.uk OR site:rvc.ac.uk OR site:rncm.ac.uk OR site:rhul.ac.uk OR site:rcs.ac.uk OR site:rcm.ac.uk OR site:rca.ac.uk OR site:cssd.ac.uk OR site:ram.ac.uk OR site:roehampton.ac.uk OR site:qub.ac.uk OR site:qmul.ac.uk OR site:qmu.ac.uk OR site:port.ac.uk']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## google search via 3rd party API's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "2020-02-17\n",
      "2020-02-21\n",
      "2020-02-24\n",
      "2020-02-27\n",
      "2020-02-28\n",
      "2020-03-02\n",
      "2020-03-03\n",
      "2020-03-04\n",
      "2020-03-05\n",
      "2020-03-06\n",
      "2020-03-09\n",
      "2020-03-10\n",
      "2020-03-11\n",
      "2020-03-12\n",
      "2020-03-13\n",
      "2020-03-16\n",
      "2020-03-17\n",
      "2020-03-18\n",
      "2020-03-19\n",
      "2020-03-20\n",
      "2020-03-21\n",
      "2020-03-23\n",
      "2020-03-24\n",
      "2020-03-25\n",
      "2020-03-26\n",
      "2020-03-27\n",
      "2020-03-30\n",
      "2020-03-31\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "end_date = datetime.datetime(2020, 4, 1, 21, 4, 5).date()\n",
    "\n",
    "# checkpoint\n",
    "d_time = datetime.datetime(2020, 2, 10, 10, 4, 5)\n",
    "date = d_time.date()\n",
    "\n",
    "\n",
    "print(len(dates))\n",
    "data = {}\n",
    "for date in dates:\n",
    "#     date1 = d_time.date()\n",
    "#     d_time += datetime.timedelta(days=1)\n",
    "#     date = d_time.date()\n",
    "    urls = []\n",
    "    print(date)\n",
    "    date_txt = \"{}-{}-{}\".format(date.year, date.month, date.day)\n",
    "\n",
    "    for index, domains in enumerate(search_domains):\n",
    "        query = 'Covid-19 ' + domains\n",
    "        params = {\n",
    "            \"query\" : query,\n",
    "            'access_key': '9e20b8648efe9aac5a8c461bcef60032',\n",
    "            \"engine\":\"google\",\n",
    "            \"auto_location\":'0',\n",
    "            \"period\":'custom',\n",
    "            \"period_start\": date.strftime('%Y-%m-%d'),\n",
    "            \"period_end\": date.strftime('%Y-%m-%d'), \n",
    "            \"num\":\"5000\"\n",
    "        }\n",
    "        response = requests.get('http://api.serpstack.com/search', params=params);\n",
    "        result = response.json()\n",
    "#         result = serpwow.get_json(params)\n",
    "        results_json = json.dumps(result, indent=2, sort_keys=True)\n",
    "        ##\n",
    "        with open('data_3/out_b{}_{}.json'.format(index, date), 'w') as f:\n",
    "            f.write(results_json)\n",
    "            \n",
    "        ##\n",
    "        urls.append(results_json)\n",
    "    data[date] = urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'organic_results' file data_1\\out_b3_2020-01-04.json\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "end_date = datetime.datetime(2020, 4, 1, 21, 4, 5).date()\n",
    "\n",
    "\n",
    "path = r'data_1/*.json'\n",
    "dates = []\n",
    "data = []\n",
    "for file in glob.glob(path):\n",
    "    tmp = json.loads(open(file).read())\n",
    "    date = tmp['search_parameters']['time_period_max']\n",
    "    try:\n",
    "#         final_records = []\n",
    "#         if len(tmp['organic_results']) == 50:\n",
    "#             dt = datetime.datetime.fromisoformat(date + ' 10:10:10').date()\n",
    "#             if dt < end_date:\n",
    "# #                 print(date)\n",
    "#                 dates.append(dt)\n",
    "        records = tmp['organic_results']\n",
    "        for x in records:\n",
    "            info = {}\n",
    "            info['search_date'] = date\n",
    "            info['cached_page_link'] = x['cached_page_link'] if 'cached_page_link' in x.keys() else '' \n",
    "            info['title'] = x['title']\n",
    "            info['snippet'] = x['snippet']\n",
    "            info['link'] = x['link']\n",
    "            info['domain'] = x['domain']\n",
    "            data.append(info)\n",
    "    except KeyError as e:\n",
    "        print(e, 'file', file)\n",
    "dates\n",
    "df3 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3]) #df for data from data_1 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18139, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13548, 6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.drop_duplicates(subset=['title', 'link', 'search_date'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('search_data_v3.csv', index='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting content from webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import logging\n",
    "from tika import parser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('search_data_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-01 15:05:03,637 [MainThread  ] [INFO ]  Retrieving https://www.aclweb.org/anthology/K19-1079.pdf to /tmp/anthology-k19-1079.pdf.\n",
      "INFO:tika.tika:Retrieving https://www.aclweb.org/anthology/K19-1079.pdf to /tmp/anthology-k19-1079.pdf.\n"
     ]
    }
   ],
   "source": [
    "link = 'https://www.aclweb.org/anthology/K19-1079.pdf'\n",
    "pdfFile = parser.from_file(link)\n",
    "content = pdfFile[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do Massively Pretrained Language Models Make Better Storytellers?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfFile['metadata']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current index 2500\n",
      "current index 3000\n",
      "current index 3500\n",
      "current index 4000\n",
      "current index 4500\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import logging\n",
    "from tika import parser\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "df = pd.read_csv('search_data_v3.csv')\n",
    "df['page_title'] = ''\n",
    "df['page_text'] = ''\n",
    "df['page_type'] = ''\n",
    "\n",
    "starting_index = 1529\n",
    "ending_index = 5000\n",
    "for index, row in df.iloc[starting_index:ending_index].iterrows():\n",
    "    \n",
    "    if index >starting_index and index % 2000 == 0: \n",
    "        df.to_csv('search_results_v4_checkpoint_{}'.format(index))\n",
    "    if index % 500 == 0:\n",
    "        print('current index', index)\n",
    "    try:\n",
    "        df.at[index,'page_type'] = 'pdf'\n",
    "        if link.endswith('.pdf'):\n",
    "            row['page_type'] = 'pdf'\n",
    "            pdfFile = parser.from_file(link)\n",
    "            text = pdfFile[\"content\"]\n",
    "            tile = pdfFile['metadata']['title'] if 'title' in pdfFile['metadata'] else ''\n",
    "        else:\n",
    "            df.at[index,'page_type'] = 'html'\n",
    "            html_content = requests.get(link).text\n",
    "            soup = BeautifulSoup(html_content, \"html\")\n",
    "            \n",
    "            if soup.title:\n",
    "                title = soup.title.string\n",
    "            else: \n",
    "                title = ''\n",
    "            # kill all script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()    # rip it out\n",
    "\n",
    "            # get text\n",
    "            text = soup.get_text()\n",
    "            # break into lines and remove leading and trailing space on each\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            # break multi-headlines into a line each\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            # drop blank lines\n",
    "            text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "        df.at[index,'page_title'] = title\n",
    "        df.at[index,'page_text'] = text\n",
    "    except requests.exceptions.SSLError as e:\n",
    "        pass\n",
    "    except Exception as e: \n",
    "        logging.error('index: ' + str(index) + ' ' +  traceback.format_exc())\n",
    "df.to_csv('search_results_v4_{}_{}.csv'.format(starting_index, ending_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2180"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('search_results_v4_{}_{}.csv'.format(starting_index, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139, 11)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df[df.page_text.duplicated(keep=False)]\n",
    "\n",
    "x = x.dropna(subset=['page_text'])\n",
    "x=x[x['Unnamed: 0.1'].astype(int)>5000]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8552, 8557, 8568, 8569, 8617, 8687, 8734, 8756, 8972, 9034]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(x['Unnamed: 0.1'].astype(int))[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>cached_page_link</th>\n",
       "      <th>domain</th>\n",
       "      <th>link</th>\n",
       "      <th>search_date</th>\n",
       "      <th>snippet</th>\n",
       "      <th>title</th>\n",
       "      <th>page_title</th>\n",
       "      <th>page_text</th>\n",
       "      <th>page_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>https://webcache.googleusercontent.com/search?...</td>\n",
       "      <td>www.dow.cam.ac.uk</td>\n",
       "      <td>https://www.dow.cam.ac.uk/outreach/university-...</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>Feb 17, 2020 - University Taster Day (Year 9)....</td>\n",
       "      <td>University Taster Day (Year 9) | Downing Colle...</td>\n",
       "      <td>Year 9 University Taster Webinar | Downing Col...</td>\n",
       "      <td>Year 9 University Taster Webinar | Downing Col...</td>\n",
       "      <td>html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>9</td>\n",
       "      <td>https://webcache.googleusercontent.com/search?...</td>\n",
       "      <td>www.dow.cam.ac.uk</td>\n",
       "      <td>https://www.dow.cam.ac.uk/outreach/year-9-univ...</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>Feb 17, 2020 - Year 9 University Taster Webina...</td>\n",
       "      <td>Year 9 University Taster Webinar | Downing Col...</td>\n",
       "      <td>Year 9 University Taster Webinar | Downing Col...</td>\n",
       "      <td>Year 9 University Taster Webinar | Downing Col...</td>\n",
       "      <td>html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12</td>\n",
       "      <td>https://webcache.googleusercontent.com/search?...</td>\n",
       "      <td>www.cranfield.ac.uk</td>\n",
       "      <td>https://www.cranfield.ac.uk/research/phd/intel...</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>Feb 17, 2020 - artificial intelligence, machin...</td>\n",
       "      <td>Intelligent Dynamic Flood Response and Recover...</td>\n",
       "      <td>403 - Forbidden: Access is denied.</td>\n",
       "      <td>403 - Forbidden: Access is denied. Server Erro...</td>\n",
       "      <td>html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>https://webcache.googleusercontent.com/search?...</td>\n",
       "      <td>blog.derby.ac.uk</td>\n",
       "      <td>https://blog.derby.ac.uk/2020/02/being-a-unive...</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>Feb 17, 2020 - Ever wondered what it's like to...</td>\n",
       "      <td>What it's like to be a university residential ...</td>\n",
       "      <td>403 Forbidden</td>\n",
       "      <td>403 Forbidden 403 Forbidden nginx</td>\n",
       "      <td>html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>15</td>\n",
       "      <td>http://webcache.googleusercontent.com/search?q...</td>\n",
       "      <td>0-blogs.biomedcentral.com.brum.beds.ac.uk</td>\n",
       "      <td>http://0-blogs.biomedcentral.com.brum.beds.ac....</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>Feb 17, 2020 - Nineteen studies were quantitat...</td>\n",
       "      <td>Highlights of the BMC Series: January 2020 - B...</td>\n",
       "      <td>403 Forbidden</td>\n",
       "      <td>403 Forbidden 403 Forbidden nginx</td>\n",
       "      <td>html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13471</th>\n",
       "      <td>13471.0</td>\n",
       "      <td>1522</td>\n",
       "      <td>https://webcache.googleusercontent.com/search?...</td>\n",
       "      <td>www.rncm.ac.uk</td>\n",
       "      <td>https://www.rncm.ac.uk/performance/an-evening-...</td>\n",
       "      <td>1/15/2020</td>\n",
       "      <td>Jan 16, 2020 - This event has been cancelled a...</td>\n",
       "      <td>An Evening with Hilary Mantel - CANCELLED - Ro...</td>\n",
       "      <td>An Evening with Hilary Mantel - CANCELLED - Ro...</td>\n",
       "      <td>An Evening with Hilary Mantel - CANCELLED - Ro...</td>\n",
       "      <td>html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13485</th>\n",
       "      <td>13485.0</td>\n",
       "      <td>1537</td>\n",
       "      <td>https://webcache.googleusercontent.com/search?...</td>\n",
       "      <td>www.qub.ac.uk</td>\n",
       "      <td>https://www.qub.ac.uk/courses/undergraduate/20...</td>\n",
       "      <td>1/15/2020</td>\n",
       "      <td>Jan 15, 2020 - ... this degree. As a result of...</td>\n",
       "      <td>Midwifery Sciences (BSC HONS) B720 | Courses ....</td>\n",
       "      <td>Midwifery Sciences (BSC HONS) B720 | Courses |...</td>\n",
       "      <td>Midwifery Sciences (BSC HONS) B720 | Courses |...</td>\n",
       "      <td>html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13486</th>\n",
       "      <td>13486.0</td>\n",
       "      <td>1538</td>\n",
       "      <td>https://webcache.googleusercontent.com/search?...</td>\n",
       "      <td>www.qub.ac.uk</td>\n",
       "      <td>https://www.qub.ac.uk/courses/undergraduate/20...</td>\n",
       "      <td>1/15/2020</td>\n",
       "      <td>Jan 15, 2020 - ... this degree. As a result of...</td>\n",
       "      <td>Mental Health Nursing (BSC HONS) B760 | Course...</td>\n",
       "      <td>Mental Health Nursing (BSC HONS) B760 | Course...</td>\n",
       "      <td>Mental Health Nursing (BSC HONS) B760 | Course...</td>\n",
       "      <td>html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13487</th>\n",
       "      <td>13487.0</td>\n",
       "      <td>1539</td>\n",
       "      <td>https://webcache.googleusercontent.com/search?...</td>\n",
       "      <td>www.qub.ac.uk</td>\n",
       "      <td>https://www.qub.ac.uk/courses/undergraduate/20...</td>\n",
       "      <td>1/15/2020</td>\n",
       "      <td>Jan 15, 2020 - ... this degree. As a result of...</td>\n",
       "      <td>Midwifery Sciences (BSC HONS) B720 | Courses |...</td>\n",
       "      <td>Midwifery Sciences (BSC HONS) B720 | Courses |...</td>\n",
       "      <td>Midwifery Sciences (BSC HONS) B720 | Courses |...</td>\n",
       "      <td>html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13488</th>\n",
       "      <td>13488.0</td>\n",
       "      <td>1540</td>\n",
       "      <td>https://webcache.googleusercontent.com/search?...</td>\n",
       "      <td>www.qub.ac.uk</td>\n",
       "      <td>https://www.qub.ac.uk/courses/undergraduate/20...</td>\n",
       "      <td>1/15/2020</td>\n",
       "      <td>Jan 15, 2020 - ... this degree. As a result of...</td>\n",
       "      <td>Mental Health Nursing (BSC HONS) B760 | Course...</td>\n",
       "      <td>Mental Health Nursing (BSC HONS) B760 | Course...</td>\n",
       "      <td>Mental Health Nursing (BSC HONS) B760 | Course...</td>\n",
       "      <td>html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3851 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 Unnamed: 0.1  \\\n",
       "5             5.0            5   \n",
       "9             9.0            9   \n",
       "12           12.0           12   \n",
       "13           13.0           13   \n",
       "15           15.0           15   \n",
       "...           ...          ...   \n",
       "13471     13471.0         1522   \n",
       "13485     13485.0         1537   \n",
       "13486     13486.0         1538   \n",
       "13487     13487.0         1539   \n",
       "13488     13488.0         1540   \n",
       "\n",
       "                                        cached_page_link  \\\n",
       "5      https://webcache.googleusercontent.com/search?...   \n",
       "9      https://webcache.googleusercontent.com/search?...   \n",
       "12     https://webcache.googleusercontent.com/search?...   \n",
       "13     https://webcache.googleusercontent.com/search?...   \n",
       "15     http://webcache.googleusercontent.com/search?q...   \n",
       "...                                                  ...   \n",
       "13471  https://webcache.googleusercontent.com/search?...   \n",
       "13485  https://webcache.googleusercontent.com/search?...   \n",
       "13486  https://webcache.googleusercontent.com/search?...   \n",
       "13487  https://webcache.googleusercontent.com/search?...   \n",
       "13488  https://webcache.googleusercontent.com/search?...   \n",
       "\n",
       "                                          domain  \\\n",
       "5                              www.dow.cam.ac.uk   \n",
       "9                              www.dow.cam.ac.uk   \n",
       "12                           www.cranfield.ac.uk   \n",
       "13                              blog.derby.ac.uk   \n",
       "15     0-blogs.biomedcentral.com.brum.beds.ac.uk   \n",
       "...                                          ...   \n",
       "13471                             www.rncm.ac.uk   \n",
       "13485                              www.qub.ac.uk   \n",
       "13486                              www.qub.ac.uk   \n",
       "13487                              www.qub.ac.uk   \n",
       "13488                              www.qub.ac.uk   \n",
       "\n",
       "                                                    link search_date  \\\n",
       "5      https://www.dow.cam.ac.uk/outreach/university-...  2020-02-17   \n",
       "9      https://www.dow.cam.ac.uk/outreach/year-9-univ...  2020-02-17   \n",
       "12     https://www.cranfield.ac.uk/research/phd/intel...  2020-02-17   \n",
       "13     https://blog.derby.ac.uk/2020/02/being-a-unive...  2020-02-17   \n",
       "15     http://0-blogs.biomedcentral.com.brum.beds.ac....  2020-02-17   \n",
       "...                                                  ...         ...   \n",
       "13471  https://www.rncm.ac.uk/performance/an-evening-...   1/15/2020   \n",
       "13485  https://www.qub.ac.uk/courses/undergraduate/20...   1/15/2020   \n",
       "13486  https://www.qub.ac.uk/courses/undergraduate/20...   1/15/2020   \n",
       "13487  https://www.qub.ac.uk/courses/undergraduate/20...   1/15/2020   \n",
       "13488  https://www.qub.ac.uk/courses/undergraduate/20...   1/15/2020   \n",
       "\n",
       "                                                 snippet  \\\n",
       "5      Feb 17, 2020 - University Taster Day (Year 9)....   \n",
       "9      Feb 17, 2020 - Year 9 University Taster Webina...   \n",
       "12     Feb 17, 2020 - artificial intelligence, machin...   \n",
       "13     Feb 17, 2020 - Ever wondered what it's like to...   \n",
       "15     Feb 17, 2020 - Nineteen studies were quantitat...   \n",
       "...                                                  ...   \n",
       "13471  Jan 16, 2020 - This event has been cancelled a...   \n",
       "13485  Jan 15, 2020 - ... this degree. As a result of...   \n",
       "13486  Jan 15, 2020 - ... this degree. As a result of...   \n",
       "13487  Jan 15, 2020 - ... this degree. As a result of...   \n",
       "13488  Jan 15, 2020 - ... this degree. As a result of...   \n",
       "\n",
       "                                                   title  \\\n",
       "5      University Taster Day (Year 9) | Downing Colle...   \n",
       "9      Year 9 University Taster Webinar | Downing Col...   \n",
       "12     Intelligent Dynamic Flood Response and Recover...   \n",
       "13     What it's like to be a university residential ...   \n",
       "15     Highlights of the BMC Series: January 2020 - B...   \n",
       "...                                                  ...   \n",
       "13471  An Evening with Hilary Mantel - CANCELLED - Ro...   \n",
       "13485  Midwifery Sciences (BSC HONS) B720 | Courses ....   \n",
       "13486  Mental Health Nursing (BSC HONS) B760 | Course...   \n",
       "13487  Midwifery Sciences (BSC HONS) B720 | Courses |...   \n",
       "13488  Mental Health Nursing (BSC HONS) B760 | Course...   \n",
       "\n",
       "                                              page_title  \\\n",
       "5      Year 9 University Taster Webinar | Downing Col...   \n",
       "9      Year 9 University Taster Webinar | Downing Col...   \n",
       "12                    403 - Forbidden: Access is denied.   \n",
       "13                                         403 Forbidden   \n",
       "15                                         403 Forbidden   \n",
       "...                                                  ...   \n",
       "13471  An Evening with Hilary Mantel - CANCELLED - Ro...   \n",
       "13485  Midwifery Sciences (BSC HONS) B720 | Courses |...   \n",
       "13486  Mental Health Nursing (BSC HONS) B760 | Course...   \n",
       "13487  Midwifery Sciences (BSC HONS) B720 | Courses |...   \n",
       "13488  Mental Health Nursing (BSC HONS) B760 | Course...   \n",
       "\n",
       "                                               page_text page_type  \n",
       "5      Year 9 University Taster Webinar | Downing Col...      html  \n",
       "9      Year 9 University Taster Webinar | Downing Col...      html  \n",
       "12     403 - Forbidden: Access is denied. Server Erro...      html  \n",
       "13                     403 Forbidden 403 Forbidden nginx      html  \n",
       "15                     403 Forbidden 403 Forbidden nginx      html  \n",
       "...                                                  ...       ...  \n",
       "13471  An Evening with Hilary Mantel - CANCELLED - Ro...      html  \n",
       "13485  Midwifery Sciences (BSC HONS) B720 | Courses |...      html  \n",
       "13486  Mental Health Nursing (BSC HONS) B760 | Course...      html  \n",
       "13487  Midwifery Sciences (BSC HONS) B720 | Courses |...      html  \n",
       "13488  Mental Health Nursing (BSC HONS) B760 | Course...      html  \n",
       "\n",
       "[3851 rows x 11 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing extracted content - merge all contents into one df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "df = pd.DataFrame()\n",
    "path = 'search_results_v4_checkpoints/*.csv'\n",
    "for file in glob.glob(path):\n",
    "#     print(file)\n",
    "    name = file.replace('search_results_v4_checkpoints/search_results_v4_checkpoint_', '')\n",
    "    name = name.replace('.csv', '')\n",
    "    indices = name.split('_')\n",
    "    ## get indices containing page info (title and body)\n",
    "    starting_index = int(indices[0])\n",
    "    ending_index = int(indices[1])\n",
    "    \n",
    "    ## load df \n",
    "    df_tmp = pd.read_csv(file)\n",
    "    df_tmp = df_tmp.iloc[starting_index:ending_index]\n",
    "    \n",
    "    df = pd.concat([df_tmp, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13548, 11)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in df.iterrows():\n",
    "    if not pd.isnull(df['page_text'].iloc[index]):\n",
    "        if link.endswith('.pdf'):\n",
    "            df.at[index,'page_type'] =  'pdf'\n",
    "        else:\n",
    "            df.at[index,'page_type'] =  'html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "df.to_json('search_results_v4.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter on Covid-19 Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep instance that contain covid-related words\n",
    "keywords_lst = ['COVID-19, coronavirus, self-isolation, infection, pandemic'\n",
    "         ,'Covid–19, pandemic, Coronavirus, Government advice, crisis',\n",
    "         'lockdown, self-isolate, cancelled'\n",
    "       ]\n",
    "keywords = []\n",
    "for words in keywords_lst:\n",
    "    keywords += words.lower().split(', ')\n",
    "keywords = set(keywords)\n",
    "\n",
    "df['contains_keyword'] = ''\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    body = str(row['page_text']).lower()\n",
    "    title = str(row['page_title']).lower()\n",
    "    if any(word in body+ ' ' + title for word in keywords):\n",
    "        df.at[index,'contains_keyword'] = 1\n",
    "    else:\n",
    "        df.at[index,'contains_keyword'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12021\n",
       "0     1527\n",
       "Name: contains_keyword, dtype: int64"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.contains_keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12021, 12)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## remove rows not matching keywords -> contains_keywords is 0\n",
    "df = df[df['contains_keyword']==1]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save rows only containing covid keywrods to v5\n",
    "df.to_csv('search_results_v5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_datetime(df['search_date'], format='%Y-%m-%d')\n",
    "## convert string to date format\n",
    "dates = []\n",
    "for index, row in df.iterrows(): \n",
    "    \n",
    "    x = row['search_date']\n",
    "    try:\n",
    "        d = datetime.datetime.strptime(x, '%Y-%m-%d').date()\n",
    "    except ValueError as e:\n",
    "        d = datetime.datetime.strptime(x, '%m/%d/%Y').date()\n",
    "    \n",
    "    df.at[index,'search_date'] = d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11140, 12)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## retain records dated prior to april 1\n",
    "\n",
    "date = datetime.datetime(2020, 4, 1, 21, 4, 5).date()\n",
    "df = df[df['search_date'] <date]\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2020, 2, 17)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('search_results_v6.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text_page to sents and create an index for senteces -> doc_id (doci_id is index from original df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valueError, using nltk 6037\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from spacy.lang.en import English\n",
    "nlp = English(disable=['ner', 'textcat'])\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)\n",
    "doc = nlp(x)\n",
    "list(doc.sents)\n",
    "df_sents = pd.DataFrame()\n",
    "sents_idx = []\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    text = row['page_text']\n",
    "    if type(text) == float: ## nan\n",
    "        sents_idx += [('', index)]\n",
    "    else:\n",
    "        try:\n",
    "            doc = nlp(text)\n",
    "            sents = list(doc.sents)\n",
    "            # map each list of sents to doc index\n",
    "            sents = [(sent.string, index) for sent in sents]\n",
    "        except ValueError as e:\n",
    "            sents = nltk.sent_tokenize(text)\n",
    "            sents = [(sent, index) for sent in sents]\n",
    "            print('valueError, using nltk', index)\n",
    "            \n",
    "        sents_idx += sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416517, 2)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_sents = pd.DataFrame(sents_idx, columns=['sent', 'doc_id'])\n",
    "df_sents.to_hdf('sents_idx.hdf', key='sents_idx')\n",
    "df_sents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data processing with USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_sents = pd.read_hdf('sents_idx.hdf', key='sents_idx')\n",
    "\n",
    "## break into 2 indices\n",
    "breakpoint = int(df_sents.shape[0]/2)\n",
    "df_sents_1 = df_sents.iloc[:breakpoint]\n",
    "df_sents_2 = df_sents.iloc[breakpoint:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sent      Women in STEM Webinair | Downing College Cambr...\n",
       "doc_id                                                    0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sents_1.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot product query vs embedded list\n",
    "def sims(data_emb, input_text):\n",
    "    embd = embed([input_text])[0].numpy()\n",
    "    return np.dot(data_emb, embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_UserObject' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-12a22a2b52d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: '_UserObject' object has no attribute 'device'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13084\n"
     ]
    }
   ],
   "source": [
    "# embed titles \n",
    "titles = df.page_title.to_list()\n",
    "titles = [title if type(title)==str else '' for title in titles]\n",
    "p = [0 if t=='' else 1 for t in titles]\n",
    "p=np.array(p)\n",
    "print(np.count_nonzero(p))\n",
    "\n",
    "titles_emb = embed(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "## embed body - index 1\n",
    "sents_emb_1 = embed(df_sents_1.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_emb_2 = embed(df_sents_2.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\tArts and Health Journal\r\n",
      " 0.20829864\n",
      "\r\n",
      "\tWelcome and Orientation Activity\r\n",
      " 0.23332433\n",
      "Virtual Panel: Coronavirus and Conservation | Conservation Research Institute 0.21787846\n",
      "Environmental Health and Preventive Medicine | Home 0.30716363\n",
      "University of Essex - Enhanced Protection Measures «  Vice-Chancellor 0.29392666\n",
      "Care and Support | CJD 0.2477134\n",
      "Mitigation and Extenuating Circumstances 0.25607383\n",
      "Digital Signage Content Guidelines 0.30545795\n",
      "Facilities and Instruments 0.21730116\n",
      "Aims and Assessment 0.23061979\n",
      "Student Protection Plan 0.20849241\n",
      "Policy and Impact 0.30500403\n",
      "Refund policy 0.20789672\n",
      "Change: Critical Understandings, Practices and Action 0.2572924\n",
      "Mindfulness and counselling short courses 0.21104819\n",
      "Policy and Guidance on Research Degrees | Academic Support 0.30499053\n",
      "Learning and Assessment 0.23480977\n",
      "Data Protection and Privacy Group (Disestablished) 0.2552428\n",
      "Health and Safety Services Homepage 0.26678556\n",
      "Atrial Fibrillation Management and Stroke Prevention 0.21355356\n",
      "Pandemic Ethics: Infectious Pathogen Control Measures and Moral Philosophy | Practical Ethics 0.25235227\n",
      "People and Culture - People and Culture 0.22345518\n",
      "Supervision and monitoring - COVID-19 0.29627728\n",
      "PGR Guidance 0.32287768\n",
      "Data and Guidelines - Grey literature - LibGuides at Newcastle University 0.2120104\n",
      "Learning and Teaching  0.21320334\n",
      "Coronavirus Advice - U/G News and Updates 0.21847601\n",
      "Women peace and security in the time of corona | LSE Women, Peace and Security blog 0.21487652\n",
      "Current policy and guidance document library | Society of Radiographers 0.22630468\n",
      "Covid-19 wellbeing support and guidance | Bournemouth University 0.20015056\n",
      "Covid-19 wellbeing support and guidance | Bournemouth University 0.20015056\n",
      "Law Tripos Parts IA and IB | Faculty of Law 0.2124418\n",
      "\r\n",
      "\tAssessment principles and policies | Coronavirus | King’s College London\r\n",
      " 0.26177624\n",
      "Where and how to apply for ethical review | Research Support 0.21400116\n",
      "Research and Publications 0.20770907\n",
      "External Educational Resources and Government Advice — OxSTaR 0.23414752\n",
      "Safety Net policy | York St John University  0.2355374\n",
      "Coronavirus (COVID-19): Safety Office updates and advice | Safety Office 0.2050656\n",
      "Ethical, Legal and Political Dimensions of International Intervention | University of Surrey 0.21252814\n",
      "Crime prevention and personal safety advice at UCL | Students - UCL – University College London 0.23608932\n",
      "Reg. 10 Examination Regulations 0.22167112\n",
      "Policies and Procedures - Student and Staff Wellbeing - Newcastle University 0.2190713\n",
      "Book Review: The Oxford Handbook on Women, Peace and Security edited by Sara E. Davies and Jacqui True | EUROPP 0.22752036\n",
      "Initiatives 0.2813852\n"
     ]
    }
   ],
   "source": [
    "query = 'guidelines and protective measures'; threshold = 0.2\n",
    "results = sims(titles_emb, query)\n",
    "\n",
    "# Get the indexes/indices of elements greater than 4 \n",
    "idx = np.where( results> threshold)[0]\n",
    "# idx = array([4, 5, 6, 7, 8])\n",
    "\n",
    "# Get the elements of the array that are greater than 4\n",
    "values = results[ results> threshold]\n",
    "# elts = array([5, 6, 7, 8, 9])\n",
    "\n",
    "\n",
    "for i, v in zip(idx, values):\n",
    "    print(titles[i], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999976], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = results[ results> 0.8]\n",
    "values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
